{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d85b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Customize column names\n",
    "col_names = ['ID','Diagnosis','radius','texture','perimeter','area','smoothness','compactness',\n",
    "             'concavity','concave_pts','symmetry','fractal_dim','radius_std','texture_std',\n",
    "             'perimeter_std','area_std', 'smoothness_std','compactness_std','concavity_std',\n",
    "             'concave_pts_std','symmetry_std','fractal_dim_std','radius_ext','texture_ext',\n",
    "             'perimeter_ext','area_ext','smoothness_ext','compactness_ext','concavity_ext',\n",
    "             'concave_pts_ext','symmetry_ext','fractal_dim_ext']\n",
    "# Import data\n",
    "df = pd.read_table('wdbc.data',sep=',',names = col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaled data\n",
    "scaled_df = pd.read_table('scaled_data',sep=',',index_col = 0)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine distributions for each feature\n",
    "num_fts = [i for i in num_cols if i != 'ID' and i != 'Diagnosis']\n",
    "df.hist(column = num_fts, grid = False, xlabelsize = 6, layout = (9,3), figsize = (25,40));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix - separate matrix for means, standard errors, and extremes\n",
    "fig,ax = plt.subplots(3, 1, figsize=(8,24))\n",
    "fig.subplots_adjust(wspace = 8)\n",
    "\n",
    "sns.heatmap(ax = ax[0], data = df[means].corr(numeric_only = True).abs(), fmt='.3f', annot = True)\n",
    "ax[0].tick_params(labelrotation = 30)\n",
    "sns.heatmap(ax = ax[1], data = df[stds].corr(numeric_only = True), fmt='.3f', annot = True)\n",
    "ax[1].tick_params(labelrotation = 30)\n",
    "sns.heatmap(ax = ax[2], data = df[exts].corr(numeric_only = True), fmt='.3f', annot = True)\n",
    "ax[2].tick_params(labelrotation = 30)\n",
    "\n",
    "ax[0].set_title('Correlation matrix for means')\n",
    "ax[1].set_title('Correlation matrix for std errors')\n",
    "ax[2].set_title('Correlation matrix for extremes')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f13fa3",
   "metadata": {},
   "source": [
    "Strongest correlations between radius/perimeter/area which makes sense geometrically. Area especially seems unnecessary since the original research actually says that area was just a function of a 2D area measurement + half perimeter.\n",
    "\n",
    "Also strong correlations between compactness/concavity/concave points (and correlated with the three geometric features). Original research indicates that compactness is a combination (perimeter^2 / area) so can consider dropping a feature here.\n",
    "\n",
    "Will be testing different feature subsets later on in modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test subset (removing most highly correlated features) to view correlation\n",
    "# between other features, then creating a new correlation matrix\n",
    "subset01 = ['Diagnosis','texture','smoothness','symmetry','fractal_dim','texture_std',\n",
    "           'smoothness_std','symmetry_std','fractal_dim_std','texture_ext','smoothness_ext',\n",
    "           'symmetry_ext','fractal_dim_ext']\n",
    "df_bool = df.loc[:, subset01]\n",
    "df_bool['Diagnosis'] = df_bool['Diagnosis'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "sns.heatmap(df_bool.corr(numeric_only = True).abs(), fmt='.2f', annot = True)\n",
    "plt.title('Correlation matrix for test subset01')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5345e0",
   "metadata": {},
   "source": [
    "Helps narrow down features for another possible subset - texture and texture_ext are highly correlated (the definition of the 'texture' measurement in the original research was vague). Same for smoothness, symmetry, and fractal dimension. Fractal dimension is also pretty highly correlated between mean and std error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean and std error correlations for 3 features\n",
    "par_corrs = ['smoothness','symmetry','fractal_dim']\n",
    "\n",
    "for f in par_corrs:\n",
    "    ext = f + '_ext'\n",
    "    sns.scatterplot(x = f, y = ext, alpha = .2, data = df, hue = 'Diagnosis', s = 20).set_title(f + ' parameters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e82ba",
   "metadata": {},
   "source": [
    "Potential pattern with fractal dimension - negative (B) cases correlated with lower worst fractal dimension (as compared to means). The original research explains that fractal dimension is approximated using the slope (on a log scale) of coastline approximations of the perimeter, in decreasing precision (and therefore decreasing perimeter). Knowing this, it's surprising that fractal dimension and perimeter aren't more closely correlated. But it's plausible that contour irregularity might be fairly independent of actual perimeter measurements - for instance, nuclei with more extreme irregularity (concavity?) would shrink faster in a sequence of coastline approximations than nuclei with equal perimeter but less extreme irregularity (so probably larger overall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4574f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some collinearity is present - using a PCA to potentially determine another subset\n",
    "pca = df.drop('ID', axis = 1).reset_index(drop = True).set_index('Diagnosis')\n",
    "pca_idx = pca.index\n",
    "pca_scal = pd.DataFrame(scale(pca), columns = pca.columns)\n",
    "pca_scal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scaling\n",
    "pca_scal.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca_fit = PCA().fit(pca_scal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621923bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot components vs. variance explained\n",
    "plt.subplots(figsize = (10,5))\n",
    "\n",
    "plt.plot(pca_fit.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Variance explained')\n",
    "plt.title('Cumulative variance explained by PCA components')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf46d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display actual percentages\n",
    "variance = []\n",
    "\n",
    "for comp in range(0,20):\n",
    "    variance.append(str(comp) + \": \" +\n",
    "                    str(round(pca_fit.explained_variance_ratio_.cumsum()[comp], 4)))\n",
    "display(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7e33e",
   "metadata": {},
   "source": [
    "It looks like the first 13 components explain over 98% of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38702fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from components - not precise, but useful as a possible subset for use\n",
    "# in modelling later\n",
    "comp_fts = pd.DataFrame(pca_fit.components_, columns = pca.columns).T\n",
    "\n",
    "ft_counts = []\n",
    "\n",
    "for i in range(0,14):\n",
    "    fts = comp_fts.iloc[:, i].round(decimals = 3).sort_values(ascending=False)\n",
    "    display(fts[0:2])\n",
    "    ft_counts.append(fts[0:1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aec715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Counter to find the most common features in the top 13 PCA components\n",
    "top_fts = []\n",
    "\n",
    "for i in range(0,14):\n",
    "    top_fts.append(ft_counts[i][0])\n",
    "\n",
    "Counter(top_fts).most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee32a4",
   "metadata": {},
   "source": [
    "Concavity std seems to be a strong explainer of overall variance in the data. PCA suggests concavity, concave points, fractal dim, fractal dim std, texture std, smoothness std are important.\n",
    "\n",
    "Interesting that standard error appears to be a good predictor? It could be useful for the model to know standard errors but seems like actual measurements should be better predictors. We will see how this subset does in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiling each feature with .qcut - intervals are not equal but we can see how\n",
    "# measurements are distributed for M (positive) cases\n",
    "df_m = df.Diagnosis == 'M'\n",
    "\n",
    "fig,axs = plt.subplots(9, 3, figsize = (30,1000))\n",
    "fig.subplots_adjust(hspace = .8)\n",
    "plt.rcParams['font.size'] = 8\n",
    "\n",
    "for ft, ax in zip(paired_num_fts, axs.ravel()):\n",
    "    qcats = pd.qcut(df[ft], q = [0, .2, .4, .6, .8, 1.],\n",
    "                    labels = ['0% - 20%','20 - 40%','40 - 60%', '60 - 80%', '80 - 100%'],\n",
    "                    duplicates = 'drop', precision = 1)\n",
    "    qbins = np.array(qcats, dtype = object)\n",
    "    ft_df = pd.DataFrame(list(zip(df_m.values, qbins))).groupby(1).mean()\n",
    "    ft_df.plot(ax = ax, kind = 'bar', title = ft + ' vs. malignancy', legend = False,\n",
    "              xlabel = 'Increasing ' + ft, ylabel = 'Proportion malignant', figsize = (12,25),\n",
    "              rot = 45, fontsize = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ID and setting Diagnosis to bool for RF (EDA) and modelling\n",
    "tts_df = df.drop('ID', axis = 1)\n",
    "tts_df['Diagnosis'] = tts_df['Diagnosis'].replace(to_replace = {'M': 1, 'B': 0})\n",
    "\n",
    "# Splitting into train and test sets - will be the same as for modelling\n",
    "X = clean_data.drop('Diagnosis', axis = 1)\n",
    "y = clean_data.Diagnosis\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 17)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4360ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - will look at feature importance, permutation importance (using train and\n",
    "# validation sets from full train set only)\n",
    "rf_i = RandomForestClassifier(random_state = 17, n_jobs = -1)\n",
    "rf1 = rf_i.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on train set: {rf1.score(X_train, y_train):.4f}\")\n",
    "print(f\"Accuracy on test set: {rf1.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one of the trees\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "estimator = rf1.estimators_[5]\n",
    "\n",
    "export_graphviz(estimator, out_file = 'tree_fi.dot', rounded = True, proportion = False,\n",
    "               precision = 2, filled = True)\n",
    "call(['dot', '-Tpng', 'tree_fi.dot', '-o', 'tree_fi.png', '-Gdpi=600'])\n",
    "Image(filename = 'tree_fi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart - feature importances\n",
    "rf1_fi = rf1.feature_importances_\n",
    "rf1_fi = 100 * (rf1_fi / rf1_fi.max())[:50]\n",
    "idx = np.argsort(rf1_fi)[:50]\n",
    "\n",
    "pos = np.arange(idx.shape[0]) + .5\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.barh(pos, rf1_fi[idx], align = 'center')\n",
    "plt.yticks(pos, X.columns[idx])\n",
    "plt.xlabel('Relative importance')\n",
    "plt.title('RF feature importances')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa9e91",
   "metadata": {},
   "source": [
    "Based on basic feature importance as calculated on the full training set, it seems that concave_pts_ext, perimeter_ext, concave_pts, radius_ext, area_ext, area, and concavity are strong predictors in RF.\n",
    "\n",
    "Also running permutation importances - feature importances are based only on gini impurity and are biased toward high-cardinality (many distinct values) features, and also are computed only on the fitted model without validating performance against unseen data. Since the goal is to make a predictive model, I need to see which features are most important for helping the model generalize to unseen data.\n",
    "\n",
    "The train/validation set used for permutation testing will be pulled from the actual train set and therefore will be a little smaller, but still sufficient - train set will be 64% of the original data and validation set will be 4%. At this point I will still hold out the actual test set so we can fairly assess model performance later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03613ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X_train\n",
    "y2 = y_train\n",
    "\n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(X2, y2, test_size = .2,\n",
    "                                                        random_state = 17)\n",
    "rf2 = rf_i.fit(X_trainb, y_trainb)\n",
    "\n",
    "print(f\"Accuracy on train set: {rf2.score(X_trainb, y_trainb):.4f}\")\n",
    "print(f\"Accuracy on test set: {rf2.score(X_testb, y_testb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff61b5",
   "metadata": {},
   "source": [
    "Lower accuracy is expected - less training data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also checking recall and f1 scores\n",
    "rf2_pred = rf2.predict(X_testb)\n",
    "rf2_predproba = rf2.predict_proba(X_testb)[:,1]\n",
    "\n",
    "rf2_r = recall_score(y_testb, rf2_pred)\n",
    "rf2_f = f1_score(y_testb, rf2_pred, average = 'weighted')\n",
    "\n",
    "rf2_cm = confusion_matrix(y_testb, rf2_pred)\n",
    "\n",
    "print('\\n Test recall:', round(rf2_r, 3), '\\n Test f1:', round(rf2_f, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "rf2_cm_disp = ConfusionMatrixDisplay(confusion_matrix = rf2_cm, display_labels = rf2.classes_)\n",
    "rf2_cm_disp.plot(cmap = plt.cm.Blues)\n",
    "plt.title('Confusion matrix, validation set RF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance to see if it confirms anything we saw in feature importance\n",
    "# First running with f1 as scoring\n",
    "result = permutation_importance(rf2, X_testb, y_testb, scoring = 'f1', n_repeats = 10, random_state = 17,\n",
    "                               n_jobs = 2)\n",
    "\n",
    "imp_sort_idx = result.importances_mean.argsort()\n",
    "\n",
    "importances = pd.DataFrame(result.importances[imp_sort_idx].T, columns = X2.columns[imp_sort_idx])\n",
    "\n",
    "# Boxplot\n",
    "ax = importances.plot.box(vert = False, whis = 10, figsize = (10,10), grid = True)\n",
    "ax.set_title(\"Permutation importances on f1\")\n",
    "ax.set_xlabel(\"Decrease in f1 score when permuted\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f894351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig,ax = plt.subplots(figsize = (10, 10))\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "ax.barh(X_testb.columns[imp_sort_idx], result.importances[imp_sort_idx].mean(axis = 1))\n",
    "ax.set_title('Permutation importances on f1')\n",
    "ax.set_xlabel(\"Decrease in f1 score when permuted\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de084be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now running with using roc_auc as scoring\n",
    "result = permutation_importance(rf2, X_testb, y_testb, scoring = 'roc_auc', n_repeats = 10, random_state = 17,\n",
    "                               n_jobs = 2)\n",
    "\n",
    "imp_sort_idx = result.importances_mean.argsort()\n",
    "\n",
    "importances = pd.DataFrame(result.importances[imp_sort_idx].T, columns = X2.columns[imp_sort_idx])\n",
    "\n",
    "# Boxplot\n",
    "ax = importances.plot.box(vert = False, whis = 10, figsize = (10,10), grid = True)\n",
    "ax.set_title(\"Permutation importances on ROC-AUC\")\n",
    "ax.set_xlabel(\"Decrease in ROC-AUC score when permuted\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32269499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig,ax = plt.subplots(figsize = (10, 10))\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "ax.barh(X_testb.columns[imp_sort_idx], result.importances[imp_sort_idx].mean(axis = 1))\n",
    "ax.set_title('Permutation importances on ROC-AUC')\n",
    "ax.set_xlabel(\"Decrease in ROC-AUC score when permuted\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82b2e2",
   "metadata": {},
   "source": [
    "Based on permutation testing, it seems that concave_pts_ext, texture, fractal_dim, fractal_dim_ext, smoothness_ext, symmetry_ext, and area are strong predictors.\n",
    "\n",
    "concave_pts_ext, perimeter_ext, radius_ext, concave_pts, concavity, area_ext, radius, texture, perimeter, concavity_ext\n",
    "\n",
    "So concave_pts_ext, perimeter_ext, radius_ext, concavity, radius, concavity_ext, and area are the predictors that are common in both feature and permutation importance with this split (both scoring methods)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
